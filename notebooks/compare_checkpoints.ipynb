{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Compare model checkpoints\n",
        "\n",
        "This notebook loads multiple models from `checkpoints/` and shows their responses side by side using a chat-style prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import gc\n",
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = \"Explain the difference between supervised learning and reinforcement learning.\"\n",
        "system_prompt = \"You are a helpful assistant.\"\n",
        "model_paths = [\n",
        "    \"checkpoints/llama3.1-8b-hard\",\n",
        "    \"checkpoints/llama3.1-8b-soft\",\n",
        "]\n",
        "max_new_tokens = 256\n",
        "hf_token = os.environ.get(\"HF_TOKEN\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4cebc266",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_chat_prompt(tokenizer, messages):\n",
        "    if getattr(tokenizer, \"chat_template\", None):\n",
        "        return tokenizer.apply_chat_template(\n",
        "            messages, tokenize=False, add_generation_prompt=True\n",
        "        )\n",
        "    lines = []\n",
        "    for msg in messages:\n",
        "        role = msg.get(\"role\", \"user\")\n",
        "        if role == \"system\":\n",
        "            prefix = \"System\"\n",
        "        elif role == \"assistant\":\n",
        "            prefix = \"Assistant\"\n",
        "        else:\n",
        "            prefix = \"User\"\n",
        "        lines.append(f\"{prefix}: {msg['content']}\")\n",
        "    lines.append(\"Assistant:\")\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "def generate_response(model_id, messages):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    prompt_text = build_chat_prompt(tokenizer, messages)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        token=hf_token,\n",
        "        dtype=torch.bfloat16,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(model.device)\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "        )\n",
        "    prompt_len = inputs[\"input_ids\"].shape[-1]\n",
        "    response = tokenizer.decode(output[0][prompt_len:], skip_special_tokens=True).strip()\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return response\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = []\n",
        "if system_prompt:\n",
        "    messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "rows = []\n",
        "for model_id in model_paths:\n",
        "    print(\"Running\", model_id)\n",
        "    response = generate_response(model_id, messages)\n",
        "    rows.append({\"model\": model_id, \"response\": response})\n",
        "\n",
        "pd.DataFrame(rows)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
